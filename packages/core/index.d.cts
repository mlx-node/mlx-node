/* auto-generated by NAPI-RS */
/* eslint-disable */
export declare class Activations {
  /**
   * Sigmoid Linear Unit (SiLU): x * sigmoid(x)
   * This is the most common activation in modern LLMs (Llama, Qwen, Phi)
   *
   * This version cleans up intermediate handles after use.
   * It works well for generation but doesn't preserve the computation graph for autograd.
   * Use `silu_for_autograd` in training contexts that need gradient computation.
   */
  static silu(input: MxArray): MxArray
  /**
   * Gaussian Error Linear Unit (GELU)
   * Approximation: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
   */
  static gelu(input: MxArray): MxArray
  /** ReLU: max(0, x) */
  static relu(input: MxArray): MxArray
  /** Sigmoid: 1 / (1 + exp(-x)) */
  static sigmoid(input: MxArray): MxArray
  /** Softmax along the last axis */
  static softmax(input: MxArray, axis?: number | undefined | null): MxArray
  /** Log-Softmax along the specified axis */
  static logSoftmax(input: MxArray, axis?: number | undefined | null): MxArray
  /** Swish/SwiGLU: Used in gated variants */
  static swiglu(gate: MxArray, up: MxArray): MxArray
}

/**
 * The Adam optimizer
 *
 * Updates parameters using:
 * m = β₁ * m + (1 - β₁) * g
 * v = β₂ * v + (1 - β₂) * g²
 * w = w - lr * m / (√v + ε)
 */
export declare class Adam {
  /**
   * Create a new Adam optimizer
   *
   * Args:
   *   learning_rate: The learning rate (default: 1e-3)
   *   beta1: The exponential decay rate for the first moment (default: 0.9)
   *   beta2: The exponential decay rate for the second moment (default: 0.999)
   *   eps: Small constant for numerical stability (default: 1e-8)
   *   bias_correction: Whether to apply bias correction (default: false)
   */
  constructor(learningRate?: number | undefined | null, beta1?: number | undefined | null, beta2?: number | undefined | null, eps?: number | undefined | null, biasCorrection?: boolean | undefined | null)
  /** Update a single parameter */
  updateSingle(paramName: string, param: MxArray, grad: MxArray): MxArray
  /** Reset optimizer state */
  reset(): void
}

/**
 * The AdamW optimizer (Adam with decoupled weight decay)
 *
 * Updates parameters using:
 * m = β₁ * m + (1 - β₁) * g
 * v = β₂ * v + (1 - β₂) * g²
 * w = w * (1 - lr * weight_decay) - lr * m / (√v + ε)
 */
export declare class AdamW {
  /**
   * Create a new AdamW optimizer
   *
   * Args:
   *   learning_rate: The learning rate (default: 1e-3)
   *   beta1: The exponential decay rate for the first moment (default: 0.9)
   *   beta2: The exponential decay rate for the second moment (default: 0.999)
   *   eps: Small constant for numerical stability (default: 1e-8)
   *   weight_decay: Weight decay coefficient (default: 0.01)
   *   bias_correction: Whether to apply bias correction (default: false)
   */
  constructor(learningRate?: number | undefined | null, beta1?: number | undefined | null, beta2?: number | undefined | null, eps?: number | undefined | null, weightDecay?: number | undefined | null, biasCorrection?: boolean | undefined | null)
  /** Update a single parameter */
  updateSingle(paramName: string, param: MxArray, grad: MxArray): MxArray
  /** Reset optimizer state */
  reset(): void
}

/**
 * Multi-head attention with separate Q/K/V projections (Qwen3 style).
 *
 * Supports:
 * - Grouped Query Attention (GQA) with different num_heads and num_kv_heads
 * - Optional QK normalization for training stability
 * - RoPE (Rotary Position Embeddings)
 * - KV caching for efficient inference
 */
export declare class Attention {
  /**
   * Creates a new multi-head attention layer.
   *
   * # Arguments
   * * `hidden_size` - Model dimension
   * * `num_heads` - Number of query heads
   * * `num_kv_heads` - Number of key/value heads (for GQA, typically < num_heads)
   * * `head_dim` - Dimension per head (optional, defaults to hidden_size / num_heads)
   * * `rope_theta` - RoPE base frequency (default: 10000)
   * * `use_qk_norm` - Whether to use QK normalization (Qwen3 feature, default: false)
   * * `qk_norm_eps` - Epsilon for QK normalization (default: 1e-6)
   */
  constructor(hiddenSize: number, numHeads: number, numKvHeads: number, headDim?: number | undefined | null, ropeTheta?: number | undefined | null, useQkNorm?: boolean | undefined | null, qkNormEps?: number | undefined | null)
  /**
   * Forward pass of attention.
   *
   * # Arguments
   * * `x` - Input tensor, shape: (batch, seq_len, hidden_size)
   * * `mask` - Optional attention mask
   * * `cache` - Optional KV cache for incremental generation
   *
   * # Returns
   * Output tensor, shape: (batch, seq_len, hidden_size)
   */
  forward(x: MxArray, mask?: MxArray | undefined | null, cache?: KVCache | undefined | null): MxArray
  /** Debug method: Forward pass with intermediate Q/K/V states captured */
  forwardDebug(x: MxArray, mask?: MxArray | undefined | null, cache?: KVCache | undefined | null): Record<string, MxArray>
  setQProjWeight(weight: MxArray): void
  setKProjWeight(weight: MxArray): void
  setVProjWeight(weight: MxArray): void
  setOProjWeight(weight: MxArray): void
  setQNormWeight(weight: MxArray): void
  setKNormWeight(weight: MxArray): void
  getQProjWeight(): MxArray
  getKProjWeight(): MxArray
  getVProjWeight(): MxArray
  getOProjWeight(): MxArray
  getQNormWeight(): MxArray | null
  getKNormWeight(): MxArray | null
  /**
   * Forward pass with cached intermediates for backward pass.
   *
   * Returns a vector containing:
   * - [0]: output (final result)
   * - [1]: input x
   * - [2]: queries (after projection, before reshape)
   * - [3]: keys (after projection, before reshape)
   * - [4]: values (after projection, before reshape)
   * - [5]: queries (after reshape/norm/transpose/rope, ready for attention)
   * - [6]: keys (after reshape/norm/transpose/rope, ready for attention)
   * - [7]: values (after reshape/transpose, ready for attention)
   * - [8]: attention output (before transpose back)
   * - [9]: attention output (after transpose, before reshape)
   * - [10]: attention output (after reshape, before o_proj)
   *
   * Note: This does NOT support KV caching - for training only
   */
  forwardWithCache(x: MxArray): Array<MxArray>
}

/**
 * Result from batch text generation
 *
 * Contains results for N prompts × G completions per prompt.
 * Results are stored flat in arrays of length N*G, where:
 * - First G elements are completions for prompt 0
 * - Next G elements are completions for prompt 1
 * - etc.
 */
export declare class BatchGenerationResult {
  /** Get all generated token arrays (N*G arrays) */
  get tokens(): Array<MxArray>
  /** Get all log probability arrays (N*G arrays) */
  get logprobs(): Array<MxArray>
  /** Get all decoded texts (N*G strings) */
  get texts(): Array<string>
  /** Get finish reasons grouped by prompt (N arrays of G finish reasons) */
  get finishReasons(): Array<Array<string>>
  /** Get token counts grouped by prompt (N arrays of G counts) */
  get tokenCounts(): Array<Array<number>>
  /** Get number of prompts */
  get numPrompts(): number
  /** Get group size (completions per prompt) */
  get groupSize(): number
}

/**
 * Batch Key-Value cache for efficient batch inference with left-padding support.
 *
 * The BatchKVCache expects inputs to be left-padded. For example, given prompts:
 * - [1, 3, 5]
 * - [7]
 * - [2, 6, 8, 9]
 *
 * They should be padded like:
 * - [0, 1, 3, 5]
 * - [0, 0, 0, 7]
 * - [2, 6, 8, 9]
 *
 * And `left_padding = [1, 3, 0]` specifies the padding for each batch element.
 *
 * Reference: mlx-lm/mlx_lm/models/cache.py:BatchKVCache (lines 662-787)
 */
export declare class BatchKVCache {
  /**
   * Creates a new batch KV cache with left-padding information.
   *
   * # Arguments
   * * `left_padding` - Array specifying left padding for each batch element
   *
   * # Example
   * ```js
   * // Three sequences with different padding amounts
   * const cache = new BatchKVCache([1, 3, 0]);
   * ```
   */
  constructor(leftPadding: Int32Array)
  /**
   * Updates the cache with new keys and values, and returns all cached keys/values.
   *
   * # Arguments
   * * `keys` - New keys to add, shape: (batch, n_kv_heads, seq_len, head_dim)
   * * `values` - New values to add, shape: (batch, n_kv_heads, seq_len, head_dim)
   *
   * # Returns
   * Array containing [cached_keys, cached_values] including the new entries
   */
  updateAndFetch(keys: MxArray, values: MxArray): Array<MxArray>
  /**
   * Filters the cache to keep only specified batch indices.
   *
   * # Arguments
   * * `batch_indices` - Indices of batch elements to keep
   *
   * # Example
   * ```js
   * // Keep only batch elements 0 and 2
   * cache.filter([0, 2]);
   * ```
   */
  filter(batchIndices: Int32Array): void
  /**
   * Extends this cache with another cache (concatenates along batch dimension).
   *
   * # Arguments
   * * `other` - Another BatchKVCache to concatenate
   */
  extend(other: BatchKVCache): void
  /** Resets the cache, clearing all stored keys and values. */
  reset(): void
  /** Returns the current index (number of cached tokens, not accounting for padding). */
  getIdx(): number
  /** Returns the offset array (per-batch offsets). */
  getOffsets(): Array<number>
  /** Returns the left padding array. */
  getLeftPadding(): Int32Array
}

export declare class Embedding {
  /** Create a new Embedding layer */
  constructor(numEmbeddings: number, embeddingDim: number)
  /** Forward pass: look up embeddings for indices */
  forward(indices: MxArray): MxArray
  /** Load pretrained embeddings */
  loadWeight(weight: MxArray): void
  /** Get the embedding weight matrix */
  getWeight(): MxArray
  /** Set the embedding weight matrix (alias for load_weight for consistency) */
  setWeight(weight: MxArray): void
}

/**
 * Multi-head attention with fused QKV projection (Phi3/Llama style).
 *
 * More efficient than separate Q/K/V projections - uses a single matrix multiplication.
 * Supports the same features as Attention: GQA, QK normalization, RoPE, KV caching.
 */
export declare class FusedAttention {
  /**
   * Creates a new multi-head attention layer with fused QKV projection.
   *
   * # Arguments
   * * `hidden_size` - Model dimension
   * * `num_heads` - Number of query heads
   * * `num_kv_heads` - Number of key/value heads (for GQA, typically < num_heads)
   * * `head_dim` - Dimension per head (optional, defaults to hidden_size / num_heads)
   * * `rope_theta` - RoPE base frequency (default: 10000)
   * * `use_qk_norm` - Whether to use QK normalization (default: false)
   * * `qk_norm_eps` - Epsilon for QK normalization (default: 1e-6)
   */
  constructor(hiddenSize: number, numHeads: number, numKvHeads: number, headDim?: number | undefined | null, ropeTheta?: number | undefined | null, useQkNorm?: boolean | undefined | null, qkNormEps?: number | undefined | null)
  /**
   * Forward pass of fused attention.
   *
   * # Arguments
   * * `x` - Input tensor, shape: (batch, seq_len, hidden_size)
   * * `mask` - Optional attention mask
   * * `cache` - Optional KV cache for incremental generation
   *
   * # Returns
   * Output tensor, shape: (batch, seq_len, hidden_size)
   */
  forward(x: MxArray, mask?: MxArray | undefined | null, cache?: KVCache | undefined | null): MxArray
  setQkvProjWeight(weight: MxArray): void
  setOProjWeight(weight: MxArray): void
  getQkvProjWeight(): MxArray
  getOProjWeight(): MxArray
  getQNormWeight(): MxArray | null
  getKNormWeight(): MxArray | null
}

/** Result from text generation with detailed metadata */
export declare class GenerationResult {
  /** Get the decoded text (if available) */
  get text(): string | null
  /** Get the generated tokens */
  get tokens(): MxArray
  /** Get the log probabilities */
  get logprobs(): MxArray
  /** Get the finish reason ("eos" or "length") */
  get finishReason(): 'eos' | 'length'
  /** Get the number of tokens generated */
  get numTokens(): number
}

/** Gradient computation utilities for backpropagation */
export declare class Gradients {
  /**
   * Compute gradient of SiLU activation: x * sigmoid(x)
   *
   * Derivative: sigmoid(x) * (1 + x * (1 - sigmoid(x)))
   */
  static siluBackward(input: MxArray, gradOutput: MxArray): MxArray
  /**
   * Compute gradient of ReLU activation
   *
   * Derivative: mask(x > 0) * grad_output
   */
  static reluBackward(input: MxArray, gradOutput: MxArray): MxArray
  /**
   * Compute gradient of Sigmoid activation
   *
   * Derivative: sigmoid(x) * (1 - sigmoid(x)) * grad_output
   */
  static sigmoidBackward(input: MxArray, gradOutput: MxArray): MxArray
  /**
   * Compute gradient of cross-entropy loss w.r.t. logits
   *
   * Given:
   * - loss = CrossEntropy(logits, targets)
   *
   * Returns:
   * - d(loss)/d(logits) = softmax(logits) - one_hot(targets)
   *
   * This is the gradient you would use to backprop through a classifier.
   */
  static crossEntropyBackward(logits: MxArray, targets: MxArray, numClasses?: number | undefined | null): MxArray
  /**
   * Compute gradient of MSE loss w.r.t. predictions
   *
   * Given:
   * - loss = MSE(predictions, targets) = mean((predictions - targets)^2)
   *
   * Returns:
   * - d(loss)/d(predictions) = 2 * (predictions - targets) / n
   */
  static mseBackward(predictions: MxArray, targets: MxArray): MxArray
  /**
   * Compute gradient of Linear layer
   *
   * Given:
   * - forward: y = xW^T + b
   * - grad_output: gradient w.r.t. output
   *
   * Returns:
   * - [grad_x, grad_weight, grad_bias]
   *
   * Where:
   * - grad_x = grad_output @ W
   * - grad_weight = grad_output^T @ x
   * - grad_bias = sum(grad_output, axis=0) if bias exists
   */
  static linearBackward(input: MxArray, weight: MxArray, gradOutput: MxArray, hasBias: boolean): Array<MxArray>
  /**
   * Compute gradient of RMSNorm layer
   *
   * This is a complex gradient involving the normalization statistics.
   */
  static rmsNormBackward(input: MxArray, weight: MxArray, gradOutput: MxArray, eps: number): Array<MxArray>
  /**
   * Compute gradient of MLP (SwiGLU) layer
   *
   * Forward:
   * - gate = x @ W_gate^T
   * - up = x @ W_up^T
   * - hidden = silu(gate) * up
   * - out = hidden @ W_down^T
   *
   * Returns: [grad_x, grad_gate_weight, grad_up_weight, grad_down_weight]
   */
  static mlpBackward(gradOutput: MxArray, input: MxArray, gateWeight: MxArray, upWeight: MxArray, downWeight: MxArray): Array<MxArray>
  /**
   * Compute gradients for multi-head attention layer.
   *
   * This is a simplified implementation that computes gradients for the learned
   * projection weights. It uses cached activations from forward_with_cache().
   *
   * # Arguments
   * * `grad_output` - Gradient from next layer, shape: (batch, seq_len, hidden_size)
   * * `cached_values` - Cached activations from forward_with_cache():
   *   - `[0]`: input x
   *   - `[1]`: queries_proj (after Q projection, before reshape)
   *   - `[2]`: keys_proj (after K projection, before reshape)
   *   - `[3]`: values_proj (after V projection, before reshape)
   *   - `[4]`: queries_final (ready for attention)
   *   - `[5]`: keys_final (ready for attention)
   *   - `[6]`: values_final (ready for attention)
   *   - `[7]`: attention_output (before transpose back)
   *   - `[8]`: attention_output_transposed
   *   - `[9]`: attention_output_reshaped (before o_proj)
   * * `q_weight` - Query projection weight
   * * `k_weight` - Key projection weight
   * * `v_weight` - Value projection weight
   * * `o_weight` - Output projection weight
   *
   * # Returns
   * Vector of gradients: [grad_input, grad_q_weight, grad_k_weight, grad_v_weight, grad_o_weight]
   */
  static attentionBackward(gradOutput: MxArray, cachedValues: Array<MxArray>, qWeight: MxArray, kWeight: MxArray, vWeight: MxArray, oWeight: MxArray): Array<MxArray>
}

/** Gradient utilities */
export declare class GradientUtils {
  /**
   * Compute the global L2 norm of gradients
   *
   * Computes sqrt(sum of squared elements across all gradients).
   * This can be used to monitor gradient magnitudes during training.
   */
  static computeGradientNorm(gradients: Record<string, MxArray>): number
  /**
   * Clip gradients by global L2 norm
   *
   * Scales all gradients proportionally so that their global L2 norm
   * doesn't exceed max_norm. This is the standard gradient clipping
   * approach used in deep learning (same as PyTorch's clip_grad_norm_
   * and MLX's clip_grad_norm).
   */
  static clipGradNorm(gradients: Record<string, MxArray>, maxNorm: number): Record<string, MxArray>
  /**
   * Clip gradients by global L2 norm and return both clipped gradients and norm
   *
   * This combines `compute_gradient_norm` and `clip_grad_norm` into one call.
   * Use this when you need both the clipped gradients and the original norm.
   */
  static clipGradNormWithNorm(gradients: Record<string, MxArray>, maxNorm: number): [Record<string, MxArray>, number]
  /**
   * Clip gradients by value
   *
   * Clips gradient values to be within [min_val, max_val]
   */
  static clipGradValue(grad: MxArray, minVal: number, maxVal: number): MxArray
}

/**
 * GRPO Training Engine
 *
 * Complete training engine that runs entirely in Rust.
 */
export declare class GrpoTrainingEngine {
  /**
   * Create a new training engine from an existing model
   *
   * # Arguments
   * * `model` - The Qwen3 model to train (will be cloned internally)
   * * `config` - Engine configuration
   */
  constructor(model: Qwen3Model, config: GrpoEngineConfig)
  /** Register a built-in reward function */
  registerBuiltinReward(config: BuiltinRewardConfig): void
  /**
   * Run a training step with provided rewards
   *
   * This method performs the complete training cycle:
   * 1. Generate completions for each prompt (G times per prompt)
   * 2. Use provided rewards to compute advantages
   * 3. Compute GRPO loss and gradients
   * 4. Apply gradients (respecting accumulation steps)
   *
   * # Arguments
   * * `prompts` - Array of chat conversations to use as prompts
   * * `rewards` - Reward values for each completion (num_prompts * group_size)
   *
   * # Returns
   * * Training step metrics
   */
  trainStep(prompts: Array<Array<ChatMessage>>, rewards: Array<number>): Promise<EngineStepMetrics>
  /**
   * Generate completions without training
   *
   * Use this to generate completions for scoring by external reward functions.
   * Returns completion texts along with the internal token data needed for training.
   */
  generateBatch(prompts: Array<Array<ChatMessage>>): Promise<Array<string>>
  /**
   * Generate completions with all data needed for training
   *
   * Returns completion texts, tokens, log probabilities, and lengths.
   * Use this when you need to score completions externally and then train.
   */
  generateBatchForTraining(prompts: Array<Array<ChatMessage>>): Promise<GenerateBatchResult>
  /**
   * Run a training step with pre-generated completions
   *
   * This method performs training using pre-generated completions,
   * eliminating the double-generation issue.
   *
   * # Arguments
   * * `prompts` - Array of chat conversations to use as prompts
   * * `rewards` - Reward values for each completion (num_prompts * group_size)
   * * `generation_result` - Pre-generated completion data from generate_batch_for_training
   *
   * # Returns
   * * Training step metrics
   */
  trainStepWithGenerations(prompts: Array<Array<ChatMessage>>, rewards: Array<number>, generationResult: GenerateBatchResult): Promise<EngineStepMetrics>
  /**
   * Score completions using registered built-in rewards
   *
   * # Arguments
   * * `prompts` - Prompt texts (expanded to match completions)
   * * `completions` - Completion texts to score
   */
  scoreCompletions(prompts: Array<string>, completions: Array<string>): Array<number>
  /** Get current training step */
  get step(): number
  /** Get current epoch */
  get epoch(): number
  /** Start a new epoch */
  startEpoch(): void
  /** End the current epoch and get metrics */
  endEpoch(epochTimeSecs: number): EngineEpochMetrics
  /** Reset the engine for a fresh training run */
  reset(): void
  /** Check if reward registry has any rewards registered */
  get hasBuiltinRewards(): boolean
  /** Get names of registered reward functions */
  get rewardNames(): Array<string>
  /** Get current micro-step within gradient accumulation */
  get microStep(): number
  /**
   * Check if an emergency checkpoint should be saved
   * This flag is set when consecutive NaN gradients reach the threshold
   */
  get needsEmergencySave(): boolean
  /** Get current NaN gradient count */
  get nanGradientCount(): number
  /** Clear the emergency save flag (call after saving emergency checkpoint) */
  clearEmergencySaveFlag(): void
}
export type GRPOTrainingEngine = GrpoTrainingEngine

/**
 * Key-Value cache for efficient transformer inference.
 *
 * Uses pre-allocated buffers with in-place assignment to avoid O(N²) concatenation overhead.
 * Allocates memory in 256-token chunks (matching MLX-LM's step size).
 */
export declare class KVCache {
  /** Creates a new empty KV cache. */
  constructor()
  /**
   * Updates the cache with new keys and values, and returns all cached keys/values.
   *
   * # Arguments
   * * `keys` - New keys to add, shape: (batch, n_kv_heads, seq_len, head_dim)
   * * `values` - New values to add, shape: (batch, n_kv_heads, seq_len, head_dim)
   *
   * # Returns
   * Array containing [cached_keys, cached_values] including the new entries
   */
  updateAndFetch(keys: MxArray, values: MxArray): [MxArray, MxArray]
  /** Resets the cache, clearing all stored keys and values. */
  reset(): void
  /** Returns the current offset (number of cached tokens). */
  getOffset(): number
}

export declare class LayerNorm {
  /** Create a new LayerNorm layer */
  constructor(dims: number, eps?: number | undefined | null)
  /**
   * Forward pass: LayerNorm(x) = (x - mean) / sqrt(var + eps) * weight + bias
   * Uses mx.fast.layer_norm for optimal performance (single fused Metal kernel)
   */
  forward(input: MxArray): MxArray
}

export declare class Linear {
  /** Create a new Linear layer */
  constructor(inFeatures: number, outFeatures: number, useBias?: boolean | undefined | null)
  /**
   * Forward pass: y = xW^T + b
   * Uses fused addmm operation when bias is present for better performance
   */
  forward(input: MxArray): MxArray
  /** Set new weights */
  setWeight(weight: MxArray): void
  /** Set new bias */
  setBias(bias?: MxArray | undefined | null): void
  /** Get the weight matrix */
  getWeight(): MxArray
  /** Get the bias vector (if present) */
  getBias(): MxArray | null
}

export declare class Losses {
  /**
   * Cross-entropy loss
   * Expects logits of shape [batch_size, vocab_size] and targets of shape [batch_size]
   */
  static crossEntropy(logits: MxArray, targets: MxArray, numClasses?: number | undefined | null, ignoreIndex?: number | undefined | null, labelSmoothing?: number | undefined | null): MxArray
  /**
   * KL Divergence loss: KL(P || Q) = sum(P * log(P/Q))
   * Expects log probabilities for numerical stability
   */
  static klDivergence(logP: MxArray, logQ: MxArray): MxArray
  /** Mean Squared Error loss */
  static mse(predictions: MxArray, targets: MxArray): MxArray
}

/** Learning rate schedulers */
export declare class LRScheduler {
  /**
   * Linear decay scheduler
   *
   * Linearly decays learning rate from initial_lr to final_lr over total_steps
   */
  static linearDecay(initialLr: number, finalLr: number, currentStep: number, totalSteps: number): number
  /**
   * Exponential decay scheduler
   *
   * lr = initial_lr * decay_rate^(current_step / decay_steps)
   */
  static exponentialDecay(initialLr: number, decayRate: number, currentStep: number, decaySteps: number): number
  /**
   * Cosine annealing scheduler
   *
   * Uses cosine annealing to decay learning rate
   */
  static cosineAnnealing(initialLr: number, minLr: number, currentStep: number, totalSteps: number): number
  /**
   * Step decay scheduler
   *
   * Decreases learning rate by factor every step_size steps
   */
  static stepDecay(initialLr: number, factor: number, currentStep: number, stepSize: number): number
}

/**
 * Multi-Layer Perceptron with SwiGLU activation.
 *
 * Uses the gated linear unit activation popularized by models like Llama and Qwen:
 * output = down_proj(silu(gate_proj(x)) * up_proj(x))
 *
 * This is more expressive than standard FFN and is the default in modern LLMs.
 */
export declare class MLP {
  /**
   * Creates a new MLP (SwiGLU) layer.
   *
   * # Arguments
   * * `hidden_size` - Input/output dimension
   * * `intermediate_size` - Hidden dimension (typically 4x or more of hidden_size)
   */
  constructor(hiddenSize: number, intermediateSize: number)
  /**
   * Forward pass: down(silu(gate(x)) * up(x))
   *
   * Uses fused C++ implementation for maximum performance (1 FFI call vs 8).
   *
   * # Arguments
   * * `x` - Input tensor, shape: (batch, seq_len, hidden_size)
   *
   * # Returns
   * Output tensor, shape: (batch, seq_len, hidden_size)
   */
  forward(x: MxArray): MxArray
  /**
   * Forward pass with cached intermediates for backward pass
   *
   * Returns: [output, gate, up, gate_act, gated]
   * - output: final output
   * - gate: gate_proj(x)
   * - up: up_proj(x)
   * - gate_act: silu(gate)
   * - gated: gate_act * up
   */
  forwardWithCache(x: MxArray): Array<MxArray>
  setGateProjWeight(weight: MxArray): void
  setUpProjWeight(weight: MxArray): void
  setDownProjWeight(weight: MxArray): void
  getGateProjWeight(): MxArray
  getUpProjWeight(): MxArray
  getDownProjWeight(): MxArray
}

export declare class MxArray {
  static fromInt32(data: Int32Array, shape: BigInt64Array): MxArray
  static fromUint32(data: Uint32Array, shape: BigInt64Array): MxArray
  static fromFloat32(data: Float32Array, shape: BigInt64Array): MxArray
  static zeros(shape: BigInt64Array, dtype?: DType | undefined | null): MxArray
  static scalarFloat(value: number): MxArray
  static scalarInt(value: number): MxArray
  static ones(shape: BigInt64Array, dtype?: DType | undefined | null): MxArray
  static full(shape: BigInt64Array, fillValue: number | MxArray, dtype?: DType | undefined | null): MxArray
  static linspace(start: number, stop: number, num?: number | undefined | null, dtype?: DType | undefined | null): MxArray
  static eye(n: number, m?: number | undefined | null, k?: number | undefined | null, dtype?: DType | undefined | null): MxArray
  static arange(start: number, stop: number, step?: number | undefined | null, dtype?: DType | undefined | null): MxArray
  reshape(shape: BigInt64Array): MxArray
  astype(dtype: DType): MxArray
  /**
   * Create a copy of this array with a new handle.
   * This is useful for parameter loading to avoid handle aliasing issues.
   */
  copy(): MxArray
  logSoftmax(axis: number): MxArray
  exp(): MxArray
  log(): MxArray
  sum(axes?: Int32Array | undefined | null, keepdims?: boolean | undefined | null): MxArray
  mean(axes?: Int32Array | undefined | null, keepdims?: boolean | undefined | null): MxArray
  clip(minimum?: number | undefined | null, maximum?: number | undefined | null): MxArray
  minimum(other: MxArray): MxArray
  maximum(other: MxArray): MxArray
  add(other: MxArray): MxArray
  sub(other: MxArray): MxArray
  mul(other: MxArray): MxArray
  div(other: MxArray): MxArray
  addScalar(value: number): MxArray
  mulScalar(value: number): MxArray
  subScalar(value: number): MxArray
  divScalar(value: number): MxArray
  matmul(other: MxArray): MxArray
  /**
   * Fused matrix multiply-add: D = beta * C + alpha * (self @ B)
   * where self is A. More efficient than separate matmul and add operations.
   * Default: alpha=1.0, beta=1.0, giving D = C + (self @ B)
   */
  addmm(c: MxArray, b: MxArray, alpha?: number | undefined | null, beta?: number | undefined | null): MxArray
  transpose(axes?: Int32Array | undefined | null): MxArray
  take(indices: MxArray, axis: number): MxArray
  takeAlongAxis(indices: MxArray, axis: number): MxArray
  /**
   * Put values into array at specified indices along an axis
   * Equivalent to: result = array.copy(); result[..., indices] = values
   * This matches MLX's put_along_axis for efficient in-place-style updates
   */
  putAlongAxis(indices: MxArray, values: MxArray, axis: number): MxArray
  slice(starts: BigInt64Array, stops: BigInt64Array): MxArray
  /**
   * Concatenate two arrays along an axis
   * Optimized for the common binary concatenation case
   */
  static concatenate(a: MxArray, b: MxArray, axis: number): MxArray
  /**
   * Concatenate multiple arrays along an axis
   * For concatenating 3 or more arrays
   */
  static concatenateMany(arrays: Array<MxArray>, axis?: number | undefined | null): MxArray
  sort(axis?: number | undefined | null): MxArray
  argsort(axis?: number | undefined | null): MxArray
  partition(kth: number, axis?: number | undefined | null): MxArray
  argpartition(kth: number, axis?: number | undefined | null): MxArray
  eval(): void
  evalAsync(): Promise<undefined>
  size(): bigint
  ndim(): number
  shape(): BigInt64Array
  /**
   * Get a single dimension from the array shape without copying the entire shape
   * This is more efficient when you only need one dimension
   *
   * Note: axis is u32 because NAPI doesn't support usize, but internally converted to usize
   */
  shapeAt(axis: number): number
  /**
   * Get batch and sequence length for 2D arrays (common pattern in transformers)
   * More efficient than calling shape() and extracting dimensions
   */
  getBatchSeqLen(): Array<number>
  /**
   * Get batch, sequence length, and hidden size for 3D arrays (common pattern in transformers)
   * More efficient than calling shape() and extracting dimensions
   */
  getBatchSeqHidden(): Array<number>
  dtype(): DType
  /**
   * Copy entire array from GPU to CPU as Float32Array
   *
   * ⚠吅 **PERFORMANCE WARNING**: This triggers a FULL GPU→CPU memory transfer!
   *
   * **Performance impact**:
   * - Forces evaluation of lazy operations
   * - Copies entire array from GPU to CPU memory
   * - Can be extremely slow for large arrays
   *
   * **Use sparingly**:
   * - Prefer `item_float32()` for scalars
   * - Prefer `item_at_float32(index)` for single elements
   * - Only use when you truly need all array data on CPU
   *
   * **Acceptable use cases**:
   * - Test validation and assertions
   * - CPU-only operations (e.g., sorting for quantiles)
   * - Final output extraction
   */
  toFloat32(): Float32Array
  /**
   * Copy entire array from GPU to CPU as Int32Array
   *
   * ⚠吅 **PERFORMANCE WARNING**: This triggers a FULL GPU→CPU memory transfer!
   *
   * See `to_float32()` documentation for performance implications and alternatives.
   * Prefer `item_int32()` for scalars.
   */
  toInt32(): Int32Array
  /**
   * Copy entire array from GPU to CPU as Uint32Array
   *
   * ⚠吅 **PERFORMANCE WARNING**: This triggers a FULL GPU→CPU memory transfer!
   *
   * See `to_float32()` documentation for performance implications and alternatives.
   */
  toUint32(): Uint32Array
  static stack(arrays: Array<MxArray>, axis?: number | undefined | null): MxArray
  static randomUniform(shape: BigInt64Array, low: number, high: number, dtype?: DType | undefined | null): MxArray
  static randomNormal(shape: BigInt64Array, mean: number, std: number, dtype?: DType | undefined | null): MxArray
  static randomBernoulli(shape: BigInt64Array, prob: number): MxArray
  static randint(shape: BigInt64Array, low: number, high: number): MxArray
  /**
   * Sample from categorical distribution
   * Takes logits and returns sampled indices
   */
  categorical(axis?: number | undefined | null): MxArray
  equal(other: MxArray): MxArray
  notEqual(other: MxArray): MxArray
  less(other: MxArray): MxArray
  lessEqual(other: MxArray): MxArray
  greater(other: MxArray): MxArray
  greaterEqual(other: MxArray): MxArray
  logicalAnd(other: MxArray): MxArray
  logicalOr(other: MxArray): MxArray
  logicalNot(): MxArray
  where(x: MxArray, y: MxArray): MxArray
  argmax(axis: number, keepdims?: boolean | undefined | null): MxArray
  argmin(axis: number, keepdims?: boolean | undefined | null): MxArray
  max(axes?: Int32Array | undefined | null, keepdims?: boolean | undefined | null): MxArray
  min(axes?: Int32Array | undefined | null, keepdims?: boolean | undefined | null): MxArray
  prod(axes?: Int32Array | undefined | null, keepdims?: boolean | undefined | null): MxArray
  var(axes?: Int32Array | undefined | null, keepdims?: boolean | undefined | null, ddof?: number | undefined | null): MxArray
  std(axes?: Int32Array | undefined | null, keepdims?: boolean | undefined | null, ddof?: number | undefined | null): MxArray
  logsumexp(axes?: Int32Array | undefined | null, keepdims?: boolean | undefined | null): MxArray
  cumsum(axis: number): MxArray
  cumprod(axis: number): MxArray
  pad(padWidth: Int32Array, constantValue: number): MxArray
  roll(shift: number, axis: number): MxArray
  split(indicesOrSections: number, axis?: number | undefined | null): Array<MxArray>
  tile(reps: Int32Array): MxArray
  repeat(repeats: number, axis: number): MxArray
  squeeze(axes?: Int32Array | undefined | null): MxArray
  expandDims(axis: number): MxArray
  broadcastTo(shape: BigInt64Array): MxArray
  abs(): MxArray
  negative(): MxArray
  sign(): MxArray
  sqrt(): MxArray
  square(): MxArray
  power(other: MxArray): MxArray
  sin(): MxArray
  cos(): MxArray
  tan(): MxArray
  sinh(): MxArray
  cosh(): MxArray
  tanh(): MxArray
  floor(): MxArray
  ceil(): MxArray
  round(): MxArray
  floorDivide(other: MxArray): MxArray
  remainder(other: MxArray): MxArray
  reciprocal(): MxArray
  arcsin(): MxArray
  arccos(): MxArray
  arctan(): MxArray
  log10(): MxArray
  log2(): MxArray
  log1p(): MxArray
}

/** NAPI-exported reward registry wrapper */
export declare class NativeRewardRegistry {
  /** Create a new reward registry */
  constructor()
  /** Register a built-in reward function */
  register(config: BuiltinRewardConfig): void
  /** Score a single completion */
  score(prompt: string, completion: string): number
  /** Score a batch of completions */
  scoreBatch(prompts: Array<string>, completions: Array<string>): Array<number>
  /** Check if registry is empty */
  get isEmpty(): boolean
  /** Get registered reward names */
  get names(): Array<string>
  /** Set whether to normalize scores */
  setNormalize(normalize: boolean): void
}

/** Result from padding sequences with masks */
export declare class PaddedSequences {
  get padded(): MxArray
  get masks(): MxArray
}

/** Qwen3 Model with automatic differentiation support */
export declare class Qwen3Model {
  /** Create a new Qwen3 model with the given configuration */
  constructor(config: Qwen3Config)
  /**
   * Forward pass through the model
   *
   * # Arguments
   * * `input_ids` - Token IDs, shape: [batch_size, seq_len]
   *
   * # Returns
   * * Logits, shape: [batch_size, seq_len, vocab_size]
   */
  forward(inputIds: MxArray): MxArray
  /**
   * Initialize KV caches for incremental generation
   *
   * Creates one KV cache per transformer layer. Call this before starting generation.
   */
  initKvCaches(): void
  /**
   * Reset all KV caches
   *
   * Clears cached key-value states. Call this between different generation sequences.
   */
  resetKvCaches(): void
  /**
   * Forward pass with KV caching for incremental generation
   *
   * # Arguments
   * * `input_ids` - Token IDs, shape: [batch_size, seq_len]
   * * `use_cache` - Whether to use KV caching (must call init_kv_caches() first)
   *
   * # Returns
   * * Logits, shape: [batch_size, seq_len, vocab_size]
   */
  forwardWithCache(inputIds: MxArray, useCache: boolean): MxArray
  /** Get model configuration */
  getConfig(): Qwen3Config
  /** Count total number of parameters in the model */
  numParameters(): number
  /**
   * Get all model parameters as a dictionary mapping names to arrays
   *
   * This matches the TypeScript API for compatibility
   */
  getParameters(): Record<string, MxArray>
  /** Load parameters from a dictionary */
  loadParameters(params: Record<string, MxArray>): void
  /**
   * Compute forward pass and loss (for evaluation)
   *
   * # Arguments
   * * `input_ids` - Input token IDs, shape: [batch_size, seq_len]
   * * `labels` - Target token IDs, shape: [batch_size, seq_len]
   *
   * # Returns
   * * Scalar loss value
   */
  computeLoss(inputIds: MxArray, labels: MxArray): MxArray
  /**
   * Compute loss and gradients using a hybrid approach
   *
   * This implementation computes gradients for the output layers and uses
   * numerical approximations for other parameters. This is sufficient to
   * demonstrate that training works while we build out full MLX autograd integration.
   *
   * # Arguments
   * * `input_ids` - Input token IDs, shape: [batch_size, seq_len]
   * * `labels` - Target token IDs, shape: [batch_size, seq_len]
   *
   * # Returns
   * * A tuple of (loss, gradients_dict) where gradients_dict maps parameter names to gradient arrays
   *
   * # Phase 6A Status
   * Current implementation computes:
   * - ✅ Exact gradients for LM head (output layer)
   * - ⚠吅 Numerical approximations for other layers
   *
   * Future: Full MLX autograd will compute exact gradients for all 250+ parameters
   */
  computeLossAndGradients(inputIds: MxArray, labels: MxArray): [MxArray, Record<string, MxArray>]
  /**
   * Complete GRPO training step using MLX Autograd (RECOMMENDED)
   *
   * This method uses automatic differentiation to compute gradients, eliminating
   * the need for manual backward pass implementation. This is the preferred approach.
   *
   * # Arguments
   * * `prompt_tokens` - Prompt token sequences [batch_size, seq_len] (1D arrays)
   * * `completion_tokens` - Completion sequences [batch*G, completion_len] (1D arrays)
   * * `completion_logprobs` - Logprobs from generation [batch*G, completion_len] (1D arrays)
   * * `rewards` - Reward scores for each completion [batch*G]
   * * `group_size` - Number of completions per prompt (G)
   * * `config` - GRPO loss configuration
   * * `learning_rate` - Learning rate for parameter updates
   *
   * # Returns
   * * Tuple of (loss_value, metrics_dict)
   */
  trainStepGrpoAutograd(promptTokens: Array<MxArray>, completionTokens: Array<MxArray>, completionLogprobs: Array<MxArray>, rewards: Float64Array, groupSize: number, config: GrpoLossConfig, learningRate: number): [number, Record<string, number>]
  /**
   * Compute gradients only without applying them (for gradient accumulation)
   *
   * This method computes GRPO loss and gradients but does NOT update parameters.
   * Used for gradient accumulation where gradients are summed across multiple
   * micro-batches before applying them.
   *
   * # Arguments
   * * `prompt_tokens` - Prompt token sequences [batch_size, seq_len] (1D arrays)
   * * `completion_tokens` - Completion sequences [batch*G, completion_len] (1D arrays)
   * * `completion_logprobs` - Logprobs from generation [batch*G, completion_len] (1D arrays)
   * * `rewards` - Reward scores for each completion [batch*G]
   * * `group_size` - Number of completions per prompt (G)
   * * `config` - GRPO loss configuration
   *
   * # Returns
   * * Tuple of (loss_value, gradients_dict, metrics_dict)
   */
  computeGradientsOnlyGrpoAutograd(promptTokens: Array<MxArray>, completionTokens: Array<MxArray>, completionLogprobs: Array<MxArray>, rewards: Float64Array, groupSize: number, config: GrpoLossConfig): [number, Record<string, MxArray>, Record<string, number>]
  /**
   * Accumulate gradients into existing gradient dictionary
   *
   * This is a helper method for gradient accumulation. It adds new_gradients
   * to accumulated_gradients element-wise.
   *
   * # Arguments
   * * `accumulated_gradients` - Existing accumulated gradients (will be modified in-place conceptually, but returns new dict)
   * * `new_gradients` - New gradients to add
   *
   * # Returns
   * * Updated gradient dictionary with accumulated values
   */
  static accumulateGradients(accumulatedGradients: Record<string, MxArray>, newGradients: Record<string, MxArray>): Record<string, MxArray>
  /**
   * Complete GRPO training step using manual gradients (Legacy)
   *
   * This method performs a full GRPO training iteration:
   * 1. Takes completions (already generated) with their logprobs and rewards
   * 2. Computes advantages
   * 3. Computes GRPO loss and gradients
   * 4. Updates model parameters
   *
   * NOTE: Use train_step_grpo_autograd instead for automatic differentiation.
   *
   * # Arguments
   * * `prompt_tokens` - Prompt token sequences [batch_size, seq_len] (1D arrays)
   * * `completion_tokens` - Completion sequences [batch*G, completion_len] (1D arrays)
   * * `completion_logprobs` - Logprobs from generation [batch*G, completion_len] (1D arrays)
   * * `rewards` - Reward scores for each completion [batch*G]
   * * `group_size` - Number of completions per prompt (G)
   * * `config` - GRPO loss configuration
   * * `learning_rate` - Learning rate for parameter updates
   *
   * # Returns
   * * Tuple of (loss_value, metrics_dict)
   */
  trainStepGrpo(promptTokens: Array<MxArray>, completionTokens: Array<MxArray>, completionLogprobs: Array<MxArray>, rewards: Float64Array, groupSize: number, config: GrpoLossConfig, learningRate: number): [number, Record<string, number>]
  /**
   * Apply gradients to model parameters
   *
   * # Arguments
   * * `gradients` - Dictionary mapping parameter names to gradient arrays
   * * `learning_rate` - Learning rate for gradient descent
   *
   * This performs a simple SGD update: param = param - lr * grad
   * Only updates parameters that have gradients; others remain unchanged.
   */
  applyGradients(gradients: Record<string, MxArray>, learningRate: number): void
  /**
   * Generate text with log probabilities (CRITICAL for GRPO training)
   *
   * This method performs autoregressive generation with:
   * - KV caching for efficient inference
   * - Sampling (temperature, top-k, top-p, min-p)
   * - Repetition penalty to reduce repetitive text
   * - Log probability tracking for policy gradient computation
   *
   * Reference: MLX-LM generate.py:410 (logprobs = logits - mx.logsumexp(logits))
   *
   * # Arguments
   * * `input_ids` - Initial input tokens [1, seq_len] or [seq_len]
   * * `config` - Generation configuration
   *
   * # Returns
   * * GenerationResult with tokens, logprobs, finish reason, and token count
   *
   * This is the primary generation API for training workloads (e.g., GRPO).
   * Uses fused C++ implementation for maximum performance.
   * For text-to-text generation with chat messages, use `generate()` instead.
   */
  generateForTraining(inputIds: MxArray, config?: GenerationConfig | undefined | null): Promise<GenerationResult>
  /**
   * Text-to-text generation with integrated tokenization
   *
   * This is a high-level API that handles chat template formatting, tokenization,
   * generation, and decoding internally. It takes chat messages, applies the ChatML
   * template, generates tokens, and decodes them back to text.
   *
   * # Arguments
   * * `messages` - Array of chat messages with role and content
   * * `config` - Generation configuration
   *
   * # Returns
   * * GenerationResult with text, tokens, logprobs, finish reason, and token count
   *
   * # Example
   * ```typescript
   * const model = await Qwen3Model.loadPretrained("path/to/model");
   * const messages = [
   *   { role: "user", content: "What is 2+2?" }
   * ];
   * const result = await model.generate(messages, {
   *   maxNewTokens: 50,
   *   temperature: 0.8,
   *   topP: 0.95,
   * });
   * console.log(result.text); // Decoded text output
   * console.log(result.tokens); // Token IDs (for GRPO)
   * console.log(result.logprobs); // Log probabilities (for GRPO)
   * ```
   */
  generate(messages: Array<ChatMessage>, config?: GenerationConfig | undefined | null): Promise<GenerationResult>
  /**
   * Generate multiple completions for multiple prompts in batch
   *
   * This is an optimized method for GRPO training that generates G completions
   * for each of N prompts. It performs all tokenization, generation, and decoding
   * in 3 blocking tasks instead of N*(1+2G) tasks.
   *
   * # Arguments
   * * `prompts` - Array of N prompt message arrays
   * * `group_size` - Number of completions (G) to generate per prompt
   * * `config` - Generation configuration (sampling params, etc.)
   *
   * # Returns
   * * BatchGenerationResult containing N*G completions with:
   *   - tokens: Flat array of N*G token arrays
   *   - logprobs: Flat array of N*G logprob arrays
   *   - texts: Flat array of N*G decoded texts
   *   - finish_reasons: N arrays of G finish reasons
   *   - token_counts: N arrays of G token counts
   *
   * # Performance
   * For N=10 prompts, G=8 completions:
   * - Old approach: N*(1 tokenize + G generate + G decode) = 10*(1+8+8) = 170 blocking tasks
   * - New approach: 1 tokenize + N*G generate + 1 decode = 1+80+1 = 82 blocking tasks (2.1x reduction)
   *
   * # Example
   * ```typescript
   * const result = await model.generateBatch(
   *   [messages1, messages2, ...], // N prompts
   *   8,                             // G completions per prompt
   *   config
   * );
   * ```
   */
  generateBatch(prompts: Array<Array<ChatMessage>>, groupSize: number, config?: GenerationConfig | undefined | null): Promise<BatchGenerationResult>
  /**
   * Decode token IDs to text using the internal tokenizer
   *
   * Helper method for decoding generated tokens. The model must have been loaded
   * via load_pretrained() to have a tokenizer available.
   *
   * # Arguments
   * * `token_ids` - Token IDs to decode as Uint32Array
   * * `skip_special_tokens` - Whether to skip special tokens (default: true)
   *
   * # Returns
   * * Decoded text string
   */
  decode(tokenIds: Uint32Array, skipSpecialTokens?: boolean | undefined | null): Promise<string>
  /**
   * Apply chat template and encode to token IDs
   *
   * Formats messages using ChatML format and encodes to tokens.
   * The model must have been loaded via load_pretrained() to have a tokenizer available.
   *
   * # Arguments
   * * `messages` - Array of chat messages
   * * `add_generation_prompt` - Whether to add generation prompt (default: true)
   *
   * # Returns
   * * Encoded token IDs as Uint32Array
   */
  applyChatTemplate(messages: Array<ChatMessage>, addGenerationPrompt?: boolean | undefined | null): Promise<Uint32Array>
  /**
   * Load a pretrained model from disk
   *
   * This loads a model from a directory containing:
   * - config.json: Model configuration
   * - weights.mlx (optional): MLX format weights with data arrays
   * - weights.safetensors (optional): SafeTensors format (not yet supported)
   *
   * # Arguments
   * * `model_path` - Path to the model directory
   *
   * # Returns
   * * A fully initialized Qwen3Model with loaded weights
   */
  static loadPretrained(modelPath: string): Promise<Qwen3Model>
  /**
   * Save model configuration and weights to disk
   *
   * This saves:
   * - config.json: Model configuration
   * - weights.safetensors: Full model weights in SafeTensors format
   * - weights.mlx: Parameter metadata (for reference)
   *
   * # Arguments
   * * `save_path` - Directory to save the model
   */
  saveModel(savePath: string): Promise<undefined>
}

/** Qwen3 Tokenizer class with NAPI bindings */
export declare class Qwen3Tokenizer {
  /**
   * Load tokenizer from tokenizer.json file
   *
   * # Arguments
   * * `path` - Path to tokenizer.json file (default: "../.cache/assets/tokenizers/qwen3_tokenizer.json")
   *
   * # Example
   * ```typescript
   * const tokenizer = Qwen3Tokenizer.fromPretrained();
   * const tokens = tokenizer.encode("Hello, world!");
   * ```
   */
  static fromPretrained(tokenizerPath: string): Promise<Qwen3Tokenizer>
  /**
   * Encode text to token IDs
   *
   * # Arguments
   * * `text` - Text to encode
   * * `add_special_tokens` - Whether to add special tokens (default: true)
   *
   * # Returns
   * Array of token IDs as Int32Array
   *
   * # Example
   * ```typescript
   * const tokens = tokenizer.encode("Hello, world!");
   * console.log(tokens); // Int32Array [9906, 11, 1879, 0]
   * ```
   */
  encode(text: string, addSpecialTokens?: boolean | undefined | null): Promise<Uint32Array>
  /**
   * Encode multiple texts in batch
   *
   * # Arguments
   * * `texts` - Array of texts to encode
   * * `add_special_tokens` - Whether to add special tokens (default: true)
   *
   * # Returns
   * Array of Int32Arrays, one for each text
   */
  encodeBatch(texts: Array<string>, addSpecialTokens?: boolean | undefined | null): Promise<Array<Uint32Array>>
  /**
   * Decode token IDs to text
   *
   * # Arguments
   * * `token_ids` - Token IDs to decode
   * * `skip_special_tokens` - Whether to skip special tokens (default: true)
   *
   * # Returns
   * Decoded text string
   *
   * # Example
   * ```typescript
   * const text = tokenizer.decode(new Int32Array([9906, 11, 1879, 0]));
   * console.log(text); // "Hello, world!"
   * ```
   */
  decode(tokenIds: Uint32Array, skipSpecialTokens?: boolean | undefined | null): Promise<string>
  /**
   * Decode multiple token sequences in batch
   *
   * # Arguments
   * * `token_ids_batch` - Array of token ID arrays to decode
   * * `skip_special_tokens` - Whether to skip special tokens (default: true)
   *
   * # Returns
   * Array of decoded text strings
   */
  decodeBatch(tokenIdsBatch: Array<Uint32Array>, skipSpecialTokens?: boolean | undefined | null): Promise<Array<string>>
  /**
   * Apply chat template to messages and encode
   *
   * Formats messages using ChatML format:
   * <|im_start|>role
  content<|im_end|>
   *
   * # Arguments
   * * `messages` - Array of chat messages
   * * `add_generation_prompt` - Whether to add assistant prompt at end (default: true)
   *
   * # Returns
   * Encoded token IDs ready for model input
   *
   * # Example
   * ```typescript
   * const messages = [
   *   { role: "system", content: "You are a helpful assistant." },
   *   { role: "user", content: "What is 2+2?" }
   * ];
   * const tokens = tokenizer.applyChatTemplate(messages, true);
   * ```
   */
  applyChatTemplate(messages: Array<ChatMessage>, addGenerationPrompt?: boolean | undefined | null): Promise<Uint32Array>
  /** Get vocabulary size */
  vocabSize(): number
  /** Get PAD token ID */
  getPadTokenId(): number
  /** Get EOS token ID */
  getEosTokenId(): number
  /** Get BOS token ID (if exists) */
  getBosTokenId(): number | null
  /** Convert token ID to string */
  idToToken(id: number): string | null
  /** Convert token string to ID */
  tokenToId(token: string): number | null
  /** Get the special token for IM_START */
  getImStartToken(): string
  /** Get the special token for IM_END */
  getImEndToken(): string
  /** Get the special token for ENDOFTEXT (used as PAD) */
  getEndoftextToken(): string
}

export declare class RMSNorm {
  /** Create a new RMSNorm layer */
  constructor(dims: number, eps?: number | undefined | null)
  /**
   * Forward pass: RMSNorm(x) = x * weight / sqrt(mean(x^2) + eps)
   * Uses mx.fast.rms_norm for optimal performance (single fused Metal kernel)
   */
  forward(input: MxArray): MxArray
  /** Get the weight (scale) parameter */
  getWeight(): MxArray
  /** Set the weight (scale) parameter */
  setWeight(weight: MxArray): void
}

/**
 * The RMSprop optimizer
 *
 * Updates parameters using:
 * v = α * v + (1 - α) * g²
 * w = w - lr * g / (√v + ε)
 */
export declare class RMSprop {
  /**
   * Create a new RMSprop optimizer
   *
   * Args:
   *   learning_rate: The learning rate (default: 1e-2)
   *   alpha: Smoothing constant (default: 0.99)
   *   eps: Small constant for numerical stability (default: 1e-8)
   *   weight_decay: Weight decay (L2 penalty) (default: 0)
   */
  constructor(learningRate?: number | undefined | null, alpha?: number | undefined | null, eps?: number | undefined | null, weightDecay?: number | undefined | null)
  /** Update a single parameter */
  updateSingle(paramName: string, param: MxArray, grad: MxArray): MxArray
  /** Reset optimizer state */
  reset(): void
}

/**
 * Rotary Position Embedding (RoPE)
 *
 * Applies rotary position embeddings to the input tensor.
 * Commonly used in modern transformer architectures (Llama, Qwen, etc.)
 *
 * # Arguments
 * * `dims` - Number of dimensions to apply RoPE to (typically head_dim or head_dim/2)
 * * `traditional` - Whether to use traditional RoPE (false for modern implementations)
 * * `base` - Base for the frequency calculation (default: 10000.0)
 * * `scale` - Scale factor for frequencies (default: 1.0)
 */
export declare class RoPE {
  /**
   * Create a new RoPE module
   *
   * # Arguments
   * * `dims` - Number of dimensions to apply RoPE to
   * * `traditional` - Whether to use traditional RoPE (default: false)
   * * `base` - Base for frequency calculation (default: 10000.0)
   * * `scale` - Scale factor (default: 1.0)
   */
  constructor(dims: number, traditional?: boolean | undefined | null, base?: number | undefined | null, scale?: number | undefined | null)
  /**
   * Apply RoPE to input tensor
   *
   * # Arguments
   * * `x` - Input tensor with shape [..., seq_len, dims]
   * * `offset` - Position offset for KV caching (default: 0)
   *
   * # Returns
   * Tensor with same shape as input, with RoPE applied
   */
  forward(x: MxArray, offset?: number | undefined | null): MxArray
}

/**
 * Rotating Key-Value cache with fixed maximum size.
 *
 * Once the cache reaches `max_size`, old entries are evicted (except for `keep` tokens).
 * This is useful for long conversations where we want to limit memory usage while
 * keeping important context (e.g., system prompts).
 *
 * Reference: mlx-lm/mlx_lm/models/cache.py:RotatingKVCache
 */
export declare class RotatingKVCache {
  /**
   * Creates a new rotating KV cache.
   *
   * # Arguments
   * * `max_size` - Maximum number of tokens to cache
   * * `keep` - Number of initial tokens to never evict (default: 0)
   *
   * # Example
   * ```js
   * // Keep system prompt (first 10 tokens), max cache 100 tokens
   * const cache = new RotatingKVCache(100, 10);
   * ```
   */
  constructor(maxSize: number, keep?: number | undefined | null)
  /**
   * Updates the cache with new keys and values, and returns all cached keys/values.
   *
   * Uses different strategies based on sequence length:
   * - Single token (seq_len=1): In-place update with rotation
   * - Multiple tokens: Concatenation with temporal reordering
   *
   * # Arguments
   * * `keys` - New keys to add, shape: (batch, n_kv_heads, seq_len, head_dim)
   * * `values` - New values to add, shape: (batch, n_kv_heads, seq_len, head_dim)
   *
   * # Returns
   * Array containing [cached_keys, cached_values] including the new entries
   */
  updateAndFetch(keys: MxArray, values: MxArray): Array<MxArray>
  /** Resets the cache, clearing all stored keys and values. */
  reset(): void
  /** Returns the current offset (total number of tokens processed). */
  getOffset(): number
  /** Returns the maximum cache size. */
  getMaxSize(): number
  /** Returns the number of tokens to keep (never evict). */
  getKeep(): number
  /** Returns the current write index (for rotation). */
  getIdx(): number
}

/**
 * The SGD (Stochastic Gradient Descent) optimizer
 *
 * Updates parameters using:
 * v = μ * v + (1 - dampening) * g
 * w = w - lr * v
 *
 * With optional Nesterov momentum and weight decay
 */
export declare class SGD {
  /**
   * Create a new SGD optimizer
   *
   * Args:
   *   learning_rate: The learning rate (required)
   *   momentum: Momentum factor (default: 0)
   *   weight_decay: Weight decay (L2 penalty) (default: 0)
   *   dampening: Dampening for momentum (default: 0)
   *   nesterov: Whether to use Nesterov momentum (default: false)
   */
  constructor(learningRate: number, momentum?: number | undefined | null, weightDecay?: number | undefined | null, dampening?: number | undefined | null, nesterov?: boolean | undefined | null)
  /** Update a single parameter */
  updateSingle(paramName: string, param: MxArray, grad: MxArray): MxArray
  /** Reset optimizer state */
  reset(): void
}

/**
 * A tensor that tracks gradients for automatic differentiation
 *
 * This is a wrapper around MxArray that provides:
 * - Gradient tracking
 * - Automatic gradient accumulation
 * - Integration with manual backward passes
 */
export declare class Tensor {
  /** Create a tensor from float32 data */
  static fromFloat32(data: Float32Array, shape: BigInt64Array, requiresGrad?: boolean | undefined | null): Tensor
  /** Create a tensor from int32 data */
  static fromInt32(data: Int32Array, shape: BigInt64Array, requiresGrad?: boolean | undefined | null): Tensor
  /** Get the shape of the underlying data */
  dataShape(): BigInt64Array
  /** Get the shape of the gradient (if it exists) */
  gradShape(): BigInt64Array | null
  /** Check if gradient exists */
  hasGrad(): boolean
  /** Check if this tensor requires gradients */
  get requiresGrad(): boolean
  /** Set whether this tensor requires gradients */
  set requiresGrad(requiresGrad: boolean)
  /** Zero out the gradient */
  zeroGrad(): void
  /**
   * Accumulate gradient
   *
   * If gradient already exists, add to it. Otherwise, set it.
   * Note: This takes ownership of the gradient array.
   */
  accumulateGrad(grad: MxArray): void
  /** Get the shape of the tensor */
  shape(): BigInt64Array
  /** Convert data to Float32 array */
  toFloat32(): Float32Array
  /** Convert gradient to Float32 array (if it exists) */
  gradToFloat32(): Float32Array | null
  /** Convert to Int32 array */
  toInt32(): Int32Array
  /**
   * Detach this tensor from the computation graph
   *
   * Returns a new tensor with the same data but no gradient tracking
   */
  detach(): Tensor
  /** Create a tensor of zeros */
  static zeros(shape: BigInt64Array, dtype?: DType | undefined | null, requiresGrad?: boolean | undefined | null): Tensor
  /** Create a tensor of ones */
  static ones(shape: BigInt64Array, dtype?: DType | undefined | null, requiresGrad?: boolean | undefined | null): Tensor
  /** Evaluate the underlying array */
  eval(): void
}

/**
 * Transformer block combining self-attention and MLP with pre-normalization.
 *
 * Architecture (Qwen3/Llama style):
 * 1. x = x + self_attn(norm(x))  # Pre-norm + residual
 * 2. x = x + mlp(norm(x))        # Pre-norm + residual
 */
export declare class TransformerBlock {
  /**
   * Creates a new transformer block.
   *
   * # Arguments
   * * `hidden_size` - Model dimension
   * * `num_heads` - Number of attention heads
   * * `num_kv_heads` - Number of key/value heads (for GQA)
   * * `intermediate_size` - FFN hidden dimension
   * * `rms_norm_eps` - Epsilon for RMSNorm
   * * `rope_theta` - RoPE base frequency (optional)
   * * `use_qk_norm` - Whether to use QK normalization (optional)
   * * `head_dim` - Dimension per head (optional)
   */
  constructor(hiddenSize: number, numHeads: number, numKvHeads: number, intermediateSize: number, rmsNormEps: number, ropeTheta?: number | undefined | null, useQkNorm?: boolean | undefined | null, headDim?: number | undefined | null)
  /**
   * Forward pass through transformer block.
   *
   * # Arguments
   * * `x` - Input tensor, shape: (batch, seq_len, hidden_size)
   * * `mask` - Optional attention mask
   * * `cache` - Optional KV cache for incremental generation
   *
   * # Returns
   * Output tensor, shape: (batch, seq_len, hidden_size)
   */
  forward(x: MxArray, mask?: MxArray | undefined | null, cache?: KVCache | undefined | null): MxArray
  /**
   * Debug method: Forward pass with intermediate states captured
   *
   * Returns a map of intermediate activations:
   * - "after_input_norm": after input layer norm
   * - "after_attn": attention output
   * - "after_attn_residual": after attention residual connection
   * - "after_post_norm": after post-attention layer norm
   * - "after_mlp": MLP output
   * - "output": final block output
   */
  forwardDebug(x: MxArray, mask?: MxArray | undefined | null, cache?: KVCache | undefined | null): Record<string, MxArray>
  getInputLayernormWeight(): MxArray
  getPostAttentionLayernormWeight(): MxArray
  setInputLayernormWeight(weight: MxArray): void
  setPostAttentionLayernormWeight(weight: MxArray): void
}

/** Configuration for batch generation */
export interface BatchGenerationConfig {
  /** Maximum number of new tokens to generate */
  maxNewTokens?: number
  /** Sampling temperature (default: 1.0) */
  temperature?: number
  /** Top-k sampling (default: 0 = disabled) */
  topK?: number
  /** Top-p sampling (default: 1.0 = disabled) */
  topP?: number
  /** Min-p sampling (default: 0.0 = disabled) */
  minP?: number
  /** EOS token ID(s) - generation stops when any of these is generated */
  eosTokenIds?: Array<number>
  /** Pad token ID for padding shorter sequences */
  padTokenId?: number
}

/** Result from batch text generation */
export interface BatchGenerationResult {
  /**
   * Generated token IDs for each sequence
   * Flattened array: tokens for sequence i start at index i * max_gen_len
   * Padded with -1 for sequences that finished early
   */
  tokensFlat: Array<number>
  /** Shape of tokens array [batch_size, max_gen_len] */
  tokensShape: Array<number>
  /**
   * Finish reason for each sequence
   * "eos" if stopped by EOS token, "length" if hit max_tokens
   */
  finishReasons: Array<string>
  /** Number of tokens generated for each sequence */
  numTokens: Array<number>
}

/** Configuration for built-in rewards */
export interface BuiltinRewardConfig {
  /** Type of reward function */
  rewardType: BuiltinRewardType
  /** Weight for this reward (default 1.0) */
  weight?: number
  /** Allowed tool names (for ToolUse) */
  allowedTools?: Array<string>
  /** Required tags (for XmlFormat) */
  requiredTags?: Array<string>
  /** Minimum length (for Length) */
  minLength?: number
  /** Maximum length (for Length) */
  maxLength?: number
  /** Use character count vs word count (for Length) */
  useChars?: boolean
  /** Required JSON fields (for JsonSchema) */
  requiredFields?: Array<string>
  /** Whether tool call is required (for ToolUse) */
  required?: boolean
}

/** Built-in reward function types */
export declare const enum BuiltinRewardType {
  /** Tool use validation */
  ToolUse = 'ToolUse',
  /** XML format validation */
  XmlFormat = 'XmlFormat',
  /** Length-based scoring */
  Length = 'Length',
  /** JSON schema validation */
  JsonSchema = 'JsonSchema'
}

/** Chat message role */
export interface ChatMessage {
  /** Role: "system", "user", or "assistant" */
  role: string
  /** Message content */
  content: string
}

/**
 * Clear the MLX memory cache to prevent memory pressure buildup
 * Should be called periodically during long-running operations
 */
export declare function clearCache(): void

/**
 * Clip gradients by global norm.
 *
 * Computes the global L2 norm across all gradients and scales them
 * if the norm exceeds max_norm. This is the standard gradient clipping
 * technique used in deep learning to prevent gradient explosion.
 *
 * # Arguments
 * * `gradients` - Vector of gradient arrays to clip
 * * `max_norm` - Maximum allowed global norm
 *
 * # Returns
 * * Vector of clipped gradients with same shapes as inputs
 *
 * # Algorithm
 * ```text
 * global_norm = sqrt(sum(||grad_i||^2 for all grads))
 * if global_norm > max_norm:
 *     scale = max_norm / (global_norm + epsilon)
 *     clipped_grads = [grad_i * scale for all grads]
 * else:
 *     clipped_grads = gradients (unchanged)
 * ```
 */
export declare function clipGradientsByGlobalNorm(gradients: Array<MxArray>, maxNorm: number): Array<MxArray>

/**
 * Clip gradients by value (element-wise clipping).
 *
 * Clips each element of each gradient to [min_value, max_value].
 *
 * # Arguments
 * * `gradients` - Vector of gradient arrays to clip
 * * `min_value` - Minimum value
 * * `max_value` - Maximum value
 *
 * # Returns
 * * Vector of clipped gradients
 */
export declare function clipGradientsByValue(gradients: Array<MxArray>, minValue: number, maxValue: number): Array<MxArray>

/**
 * Compute advantages for GRPO from rewards
 *
 * Reference: TRL grpo_trainer.py:1567-1588
 *
 * Algorithm:
 * 1. Reshape rewards from (B*G,) to (B, G) where B=batch, G=num_generations
 * 2. Compute mean reward per group (per prompt): mean_grouped_rewards
 * 3. Advantages = rewards - mean_grouped_rewards (zero-mean per group)
 * 4. Normalize by std based on scale_rewards:
 *    - "group": Normalize by std within each group
 *    - "batch": Normalize by global std across all rewards
 *    - "none": No normalization (but still zero-mean)
 *
 * # Arguments
 * * `rewards` - Reward values, shape (B*G,) where B=batch_size, G=num_generations
 * * `num_generations` - Number of completions per prompt (G)
 * * `scale_rewards` - How to normalize: "group", "batch", or "none"
 *
 * # Returns
 * Advantages, shape (B*G,)
 */
export declare function computeAdvantages(rewards: MxArray, numGenerations: number, scaleRewards: string): MxArray

/**
 * Compute per-token entropy from logits
 *
 * Entropy H = -sum(p * log(p)) measures prediction uncertainty.
 * High entropy indicates the model is uncertain about the next token.
 *
 * # Arguments
 * * `logits` - Model logits of shape (..., vocab_size)
 *
 * # Returns
 * Entropy values of shape (...,) - last dimension (vocab) is reduced
 *
 * # Example
 * ```rust
 * // logits: [batch, seq_len, vocab_size]
 * // returns: [batch, seq_len]
 * ```
 */
export declare function computeEntropy(logits: MxArray): MxArray

export interface ConversionOptions {
  /** Input directory containing model files (config.json, model.safetensors) */
  inputDir: string
  /** Output directory for converted model */
  outputDir: string
  /** Target dtype for conversion (default: "float32") */
  dtype?: string
  /** Whether to verbose logging (default: false) */
  verbose?: boolean
}

export interface ConversionResult {
  /** Number of tensors converted */
  numTensors: number
  /** Total number of parameters */
  numParameters: number
  /** Output model path */
  outputPath: string
  /** List of converted tensor names */
  tensorNames: Array<string>
}

/**
 * Convert a HuggingFace SafeTensors model to MLX format
 *
 * This function:
 * 1. Loads SafeTensors model from input directory
 * 2. Converts all tensors to specified dtype (default: float32)
 * 3. Saves converted model to output directory
 * 4. Copies config.json and tokenizer files
 *
 * # Arguments
 * * `options` - Conversion options (input_dir, output_dir, dtype, verbose)
 *
 * # Returns
 * * ConversionResult with statistics about the conversion
 *
 * # Example
 * ```typescript
 * import { convertModel } from '../../index.cjs';
 *
 * const result = await convertModel({
 *   inputDir: '.cache/models/qwen3-0.6b',
 *   outputDir: '.cache/models/qwen3-0.6b-mlx',
 *   dtype: 'float32',
 *   verbose: true
 * });
 *
 * console.log(`Converted ${result.numTensors} tensors (${result.numParameters} parameters)`);
 * ```
 */
export declare function convertModel(options: ConversionOptions): Promise<ConversionResult>

export declare function convertParquetToJsonl(inputPath: string, outputPath: string): void

/**
 * Creates an attention mask for transformer models.
 *
 * This is a high-level helper that decides when to create a mask based on
 * sequence length and cache usage. Aligned with mlx-lm behavior.
 *
 * # Arguments
 * * `seq_len` - Sequence length (N)
 * * `use_causal` - Whether to use causal masking (for autoregressive models)
 * * `cache` - Whether KV caching is being used
 * * `offset` - Optional cache offset for incremental generation
 * * `window_size` - Optional sliding window size for local attention
 *
 * # Returns
 * * `None` if seq_len is 1 or cache is being used (no mask needed)
 * * Causal mask array if use_causal is true and seq_len > 1
 *
 * # Notes
 * - When using KV cache, mask is typically not needed because:
 *   1. Cached generation processes one token at a time (seq_len=1)
 *   2. Previous tokens are in cache, new token only attends to past
 * - For sliding window attention with cache, offset and window_size are used
 */
export declare function createAttentionMaskForTransformer(seqLen: number, useCausal: boolean, cache: boolean, offset?: number | undefined | null, windowSize?: number | undefined | null): MxArray | null

export declare const enum DType {
  Float32 = 0,
  Int32 = 1,
  Float16 = 2,
  BFloat16 = 3,
  Uint32 = 4
}

/** Metrics from a training epoch */
export interface EngineEpochMetrics {
  /** Epoch number */
  epoch: number
  /** Average loss for the epoch */
  avgLoss: number
  /** Average reward for the epoch */
  avgReward: number
  /** Total steps in the epoch */
  totalSteps: number
  /** Total tokens processed */
  totalTokens: number
  /** Time for the epoch (seconds) */
  epochTimeSecs: number
}

/** Metrics from a single training step */
export interface EngineStepMetrics {
  /** Current step number */
  step: number
  /** GRPO loss value */
  loss: number
  /** Mean reward across completions */
  meanReward: number
  /** Standard deviation of rewards */
  stdReward: number
  /** Mean advantage value */
  meanAdvantage: number
  /** Total tokens generated this step */
  totalTokens: number
  /** Whether gradients were applied */
  gradientsApplied: boolean
  /** Time for generation (ms) */
  generationTimeMs: number
  /** Time for training (ms) */
  trainingTimeMs: number
}

/** Result from generate_batch_for_training with all data needed for training */
export interface GenerateBatchResult {
  /** Generated completion texts */
  completionTexts: Array<string>
  /** Completion token IDs (flattened, concatenated) */
  completionTokens: Array<number>
  /** Completion log probabilities (flattened, concatenated) */
  completionLogprobs: Array<number>
  /** Lengths of each completion (for reconstruction) */
  completionLengths: Array<number>
}

/** Configuration for text generation */
export interface GenerationConfig {
  /** Maximum number of new tokens to generate (default: 100) */
  maxNewTokens?: number
  /** Sampling temperature (0 = greedy, higher = more random) (default: 1.0) */
  temperature?: number
  /** Top-k sampling: keep only top k tokens (0 = disabled) (default: 0) */
  topK?: number
  /** Top-p (nucleus) sampling: keep tokens with cumulative prob < p (default: 1.0) */
  topP?: number
  /** Min-p sampling: keep tokens with prob > min_p * max_prob (default: 0.0) */
  minP?: number
  /** Repetition penalty factor (1.0 = no penalty, 1.1-1.5 typical) (default: 1.0) */
  repetitionPenalty?: number
  /**
   * Number of recent tokens to consider for repetition penalty (default: 20)
   * Matches mlx-lm default. Larger values catch longer patterns but use more memory
   */
  repetitionContextSize?: number
  /** EOS token ID (generation stops when this is generated) */
  eosTokenId?: number
  /** Whether to return log probabilities (always true for GRPO) */
  returnLogprobs?: boolean
}

/** Get actively used memory in bytes (excludes cached memory) */
export declare function getActiveMemory(): number

/** Get cache memory size in bytes */
export declare function getCacheMemory(): number

/**
 * Returns a binary mask identifying tokens whose entropy exceeds a given quantile threshold.
 *
 * This function enables selective GRPO training by identifying high-uncertainty tokens.
 * The quantile threshold determines what percentage of tokens to train on:
 * - threshold=0.0: train on all non-pad tokens (0th quantile)
 * - threshold=0.5: train on top 50% highest entropy tokens (median)
 * - threshold=0.8: train on top 20% highest entropy tokens
 * - threshold=1.0: train on only the single highest entropy token
 *
 * Algorithm:
 * 1. Extract entropy values for non-padding tokens using the mask
 * 2. Compute the quantile threshold across all non-padding entropies
 * 3. Create boolean mask where entropy >= threshold
 * 4. Ensure padding tokens remain masked out
 *
 * # Arguments
 * * `entropies` - Tensor of shape (batch_size, seq_len) with per-token entropy values
 * * `mask` - Binary mask of same shape where 1=valid token, 0=padding
 * * `threshold` - Quantile threshold between 0.0 and 1.0 for selecting high-entropy tokens
 *
 * # Returns
 * Boolean mask of shape (batch_size, seq_len) where 1=train on this token
 *
 * # Example
 * ```rust
 * // Entropies: [0.1, 0.5, 0.9, 0.3, 0.7]
 * // Mask: [1, 1, 1, 1, 0] (last token is padding)
 * // Threshold: 0.5
 * // Result: trains on top 50% (2 out of 4 non-pad tokens: 0.9 and 0.7)
 * ```
 */
export declare function getHighEntropyMask(entropies: MxArray, mask: MxArray, threshold: number): MxArray

/** Get current memory limit */
export declare function getMemoryLimit(): number

/** Get peak memory usage in bytes */
export declare function getPeakMemory(): number

/** Configuration for the GRPO training engine */
export interface GrpoEngineConfig {
  /** Learning rate (default: 1e-6) */
  learningRate?: number
  /** Gradient accumulation steps (default: 1) */
  gradientAccumulationSteps?: number
  /** Maximum gradient norm for clipping (default: 1.0) */
  gradientClipNorm?: number
  /**
   * Maximum gradient value for element-wise clipping (default: 1.0)
   * This clamps individual gradient elements to [-value, value]
   */
  gradientClipValue?: number
  /** Number of completions per prompt (default: 4) */
  groupSize?: number
  /** PPO clipping epsilon (default: 0.2) */
  clipEpsilon?: number
  /** KL divergence coefficient (default: 0.0) */
  klCoef?: number
  /** Loss type: "grpo", "dapo", "dr_grpo", "bnpo" (default: "grpo") */
  lossType?: string
  /** Maximum tokens to generate (default: 256) */
  maxNewTokens?: number
  /** Sampling temperature (default: 0.8) */
  temperature?: number
  /** Top-p (nucleus) sampling (default: 0.95) */
  topP?: number
  /** Top-k sampling (optional) */
  topK?: number
  /** Repetition penalty (default: 1.1) */
  repetitionPenalty?: number
  /**
   * Steps between heavy cleanup to prevent GPU timeout (default: 25)
   * Heavy cleanup forces complete GPU drain including peak memory reset
   */
  heavyCleanupInterval?: number
  /**
   * Maximum allowed NaN gradient occurrences before stopping training (default: 100)
   * When exceeded, training will stop with an error to prevent model corruption.
   */
  maxNanGradients?: number
  /**
   * Consecutive NaN gradients that trigger emergency checkpoint (default: 5)
   * When reached, the needs_emergency_save flag is set for the TypeScript layer.
   */
  emergencySaveThreshold?: number
}

/** Configuration for GRPO loss computation */
export interface GrpoLossConfig {
  /** Lower clipping bound (default: 0.2, means clip to [1-0.2, 1+epsilon_high]) */
  epsilonLow: number
  /** Upper clipping bound (default: same as epsilon_low) */
  epsilonHigh?: number
  /** KL divergence penalty coefficient (default: 0.0, no penalty) */
  beta: number
  /** Loss aggregation type: "grpo", "bnpo", "dr_grpo", or "dapo" */
  lossType: string
  /** Importance sampling level: "token" or "sequence" */
  importanceSamplingLevel: string
  /** Maximum completion length (needed for dr_grpo) */
  maxCompletionLength?: number
  /** Total number of items in batch across all processes (needed for dapo) */
  numItemsInBatch?: number
  /** Current gradient accumulation step (for loss scaling) */
  gradientAccumulationSteps: number
}

/**
 * Heavy cleanup: synchronize, clear cache, and reset peak memory tracking
 * Use periodically (every 25-50 steps) to prevent GPU timeout in long-running training
 */
export declare function heavyCleanup(): void

/**
 * Pad variable-length float sequences to uniform length
 *
 * Takes a list of 1D float sequences (e.g., log probabilities) and pads them to the maximum length.
 *
 * # Arguments
 * * `sequences` - Vector of 1D float arrays with variable lengths
 * * `pad_value` - Value to use for padding (default: 0.0)
 *
 * # Returns
 * Padded array with shape [num_seqs, max_len]
 */
export declare function padFloatSequences(sequences: Array<MxArray>, padValue: number): MxArray

/**
 * Pad variable-length sequences to uniform length (for integers/tokens)
 *
 * Takes a list of 1D sequences and pads them to the maximum length.
 * Returns both the padded sequences and binary masks indicating real vs padded positions.
 *
 * # Arguments
 * * `sequences` - Vector of 1D arrays with variable lengths
 * * `pad_value` - Value to use for padding (default: 0)
 *
 * # Returns
 * Object with `padded` (shape: [num_seqs, max_len]) and `masks` (same shape, 1.0 for real tokens, 0.0 for padding)
 */
export declare function padSequences(sequences: Array<MxArray>, padValue: number): PaddedSequences

/** Qwen3 model configuration */
export interface Qwen3Config {
  vocabSize: number
  hiddenSize: number
  numLayers: number
  numHeads: number
  numKvHeads: number
  intermediateSize: number
  rmsNormEps: number
  ropeTheta: number
  maxPositionEmbeddings: number
  headDim: number
  useQkNorm: boolean
  tieWordEmbeddings: boolean
  padTokenId: number
  eosTokenId: number
  bosTokenId: number
}

/** Reset peak memory counter to zero */
export declare function resetPeakMemory(): void

/**
 * Configuration for sampling strategies
 * ⚡ PERFORMANCE: Made Copy to avoid cloning on every token
 */
export interface SamplingConfig {
  /** Temperature for softmax (default: 1.0). Lower = more deterministic */
  temperature?: number
  /** Number of top tokens to keep (top-k sampling). 0 = disabled */
  topK?: number
  /** Cumulative probability threshold (top-p/nucleus sampling). 1.0 = disabled */
  topP?: number
  /** Minimum probability threshold relative to max (min-p sampling). 0 = disabled */
  minP?: number
}

/**
 * Scaled dot-product attention using MLX's optimized kernel.
 *
 * Computes: O = softmax(scale * (Q @ K^T)) @ V
 *
 * # Arguments
 * * `queries` - Query tensor [batch, n_heads, seq_len, head_dim]
 * * `keys` - Key tensor [batch, n_heads, seq_len, head_dim]
 * * `values` - Value tensor [batch, n_heads, seq_len, head_dim]
 * * `scale` - Scale factor (typically 1/sqrt(head_dim))
 * * `mask` - Optional attention mask (None for no mask)
 *
 * # Returns
 * Attention output with same shape as values
 */
export declare function scaledDotProductAttention(queries: MxArray, keys: MxArray, values: MxArray, scale: number, mask?: MxArray | undefined | null): MxArray

/**
 * Scaled dot-product attention with "causal" mask mode.
 *
 * Uses MLX's optimized internal causal masking (no explicit mask array needed).
 * This is faster than passing an explicit mask because MLX handles it internally
 * with an optimized kernel.
 *
 * # Arguments
 * * `queries` - Query tensor [batch, n_heads, seq_len, head_dim]
 * * `keys` - Key tensor [batch, n_heads, kv_len, head_dim]
 * * `values` - Value tensor [batch, n_heads, kv_len, head_dim]
 * * `scale` - Scale factor (typically 1/sqrt(head_dim))
 *
 * # Returns
 * Attention output with same shape as values
 */
export declare function scaledDotProductAttentionCausal(queries: MxArray, keys: MxArray, values: MxArray, scale: number): MxArray

/**
 * Compute selective log-softmax: extract log P(token_i | context) for selected tokens only
 *
 * This is more efficient than computing full softmax when we only need probabilities
 * for a small subset of tokens (e.g., the generated completion tokens).
 *
 * Reference: TRL grpo_trainer.py _get_per_token_logps_and_entropies
 *
 * # Arguments
 * * `logits` - Model logits, shape (B, T, V) where V=vocab_size
 * * `target_ids` - Token IDs to extract probabilities for, shape (B, T)
 *
 * # Returns
 * * Log probabilities for selected tokens, shape (B, T)
 *
 * # Algorithm
 * For each position (b, t):
 *   1. Compute log-softmax: logits[b,t,:] - logsumexp(logits[b,t,:])
 *   2. Extract value at target_ids[b,t]
 */
export declare function selectiveLogSoftmax(logits: MxArray, targetIds: MxArray): MxArray

/**
 * Set memory limit (guideline for max memory use)
 * Returns the previous limit
 */
export declare function setMemoryLimit(limit: number): number

/**
 * Synchronize and clear cache - prevents GPU timeout and memory pressure
 * This is the recommended function for long-running training loops
 */
export declare function synchronizeAndClearCache(): void
